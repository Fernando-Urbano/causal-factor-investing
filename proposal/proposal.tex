\documentclass{article}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{mathtools}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{amssymb}
\usepackage{color}
\usepackage{lscape}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{hyperref}
\usepackage{fancyhdr}
\pagestyle{fancy}

\numberwithin{equation}{section}

% Define the header
\fancyfoot[R]{Fernando Urbano}
\renewcommand{\footrulewidth}{0.2pt}

\fancyhead[L]{ECMA 31380 - Causal Machine Learning}
\fancyhead[R]{Proposal Final Project}

\usepackage{graphicx}
\setlength{\parskip}{0.5em}
\setlength{\parindent}{0pt}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
\newcommand{\divider}{\vspace{1em}\hrule\vspace{1em}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{Rstyle}{
  backgroundcolor=\color{backcolour},   
  commentstyle=\color{codegreen},
  keywordstyle=\color{blue},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=true,                  
  tabsize=2,
  language=R
}

\title{ECMA 31380 - Proposal Final Project}
\author{Fernando Rocha Urbano}
\date{Autumn 2024}

% Define colors
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Setup the listings package
\lstdefinestyle{Python}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\newenvironment{colorparagraph}[1]{\par\color{#1}}{\par}
\definecolor{questioncolor}{RGB}{20, 40, 150}
\definecolor{tacolor}{RGB}{200, 0, 0}
% \documentclass[tikz,border=2mm]{standalone}
% \lstset{style=mystyle}
\usetikzlibrary{arrows.meta, positioning}


\begin{document}

\maketitle

\section{Goal}

The goal of this project is to evaluate the performance of different Double Machine Learning (DML) models in recovering the Average Treatment Effect (ATE) across various causal inference scenarios addressed in simulations.

The desired broader impact of the project is to provide actionable recommendations for selecting the most appropriate DML method based on the causal scenario, data structure, noise levels, and sample size. Additionally, we aim to create a benchmarking framework that practitioners can use to evaluate and compare causal inference estimates derived from the tested methods in different scenarios.

The DML methods compared in this study are:
\begin{itemize}
    \item LASSO.
    \item Random Forest.
    \item Neural Networks.
\end{itemize}

The causal inference scenarios are:
\begin{itemize}
    \item Backdoor adjustment.
    \item Frontdoor adjustment with and without access to mediators.
    \item Instrumental Variables with varying levels of instrument strength and correct vs. incorrect use of the IV.
\end{itemize}

We explore how each model implementation performs under varying conditions:
\begin{itemize}
    \item Relationships as linear and non-linear between covariates and treatment and between covariates and target $(g(\cdot) \ \text{and} \ m(\cdot))$.
    \item Noise levels $(\varepsilon)$.
    \item Sample size $(n)$.
    \item Size of high-dimensional impactful covariates $(d_c)$.
    \item Size of high-dimensional irrelevant covariates $(d_a)$.
\end{itemize}

For each of the scenario combinations, we show which of LASSO, Random Forest and Neural Networks provides more accurate ATE estimates based on the following metrics.
\begin{itemize}
    \item Bias of $\hat{\text{ATE}}$.
    \item Variance of $\hat{\text{ATE}}$ across repeated simulations for a given scenario.
    \item Analytical Variance of the $\hat{\text{ATE}}$.
    \item MSE (Mean Squared Error).
\end{itemize}

\section{Methodology}

\subsection{Problem Setup}

Consider a random sample ${(Y_i, T_i, X_i)}_{i=1}^n$, where:
\begin{itemize}
\item $Y_i \in \mathbb{R}$ is the outcome variable.
\item $T_i \in {0,1}$ is a binary treatment indicator.
\item $X_i \in \mathbb{R}^p$ is a vector of covariates.
\end{itemize}

Our goal is to estimate the Average Treatment Effect (ATE), defined as:
\begin{equation}
\tau = \mathbb{E}[Y_i(1) - Y_i(0)],
\label{eq:tau_hat_potential_outcome}
\end{equation}
where $Y_i(t)$ denotes the potential outcome for unit $i$ under treatment $T_i = t$.

\subsection{Identification via Conditional Expectations}

Under the Conditional Independence Assumption (CIA) and overlap conditions, the ATE can be identified as:
\begin{equation}
\tau = \mathbb{E}[\mu_1(X_i) - \mu_0(X_i)],
\label{eq:tau_hat_potential_outcome_diff_means}
\end{equation}
where $\mu_t(X_i) = \mathbb{E}[Y_i | T_i = t, X_i]$ is the conditional expectation of the outcome given treatment and covariates.

\subsection{Double Machine Learning (DML) Estimator}

The DML framework aims to estimate $\tau$ while controlling for high-dimensional or complex relationships between $Y_i$, $T_i$, and $X_i$. The key idea is to use machine learning methods to estimate the nuisance parameters and then construct an estimator for $\tau$ that is robust to estimation errors in these nuisance parameters.

\subsubsection{Nuisance Parameter Estimation}

We define the following nuisance functions:
\begin{align}
& m(X_i) = \mathbb{E}[Y_i | X_i],
\label{eq:m_x_for_target}
\\
& g(X_i) = \mathbb{E}[T_i | X_i],
\label{eq:g_x_for_treatment}
\\
& \pi(X_i) = \mathbb{P}(T_i = 1 | X_i) \quad \text{(propensity score)}
\label{eq:pi_x_for_treatment}
\end{align}

These functions can be estimated using flexible machine learning methods such as LASSO, Random Forests, or Neural Networks.

Under mild regularity conditions and appropriate rates of convergence for the nuisance estimators, the DML estimator is root-n consistent and asymptotically normal. It provides valid confidence intervals and hypothesis tests even when using complex machine learning methods for $\hat{m}(X)$ and $\hat{g}(X)$.

\subsubsection{Orthogonal Score Function}

To achieve robustness, we construct an orthogonal score function $\psi(Y_i, T_i, X_i; \eta)$, where $\eta = (m, g)$ represents the nuisance parameters. The orthogonal score satisfies the Neyman orthogonality condition, which ensures that small estimation errors in $\eta$ have a negligible first-order impact on the estimation of $\tau$.

A common choice for the orthogonal score is:
\begin{equation}
\psi(Y_i, T_i, X_i; \eta) = \left( \frac{T_i - g(X_i)}{\pi(X_i)(1 - \pi(X_i))} \right) (Y_i - m(X_i)) + \left( m_1(X_i) - m_0(X_i) \right) - \tau,
\label{eq:orthogonal_score}
\end{equation}

where $g(X_i) = \pi(X_i)$ in case of binary treatment and:

\begin{equation}
m_t(X_i) = \mathbb{E}[Y_i | T_i = t, X_i]  \quad \text{for} \ t \in \{0,1\}
\label{eq:orthogonal_score_m_t}
\end{equation}

\subsubsection{Estimation Procedure}

The estimation proceeds in several steps:
\begin{enumerate}
\item Estimate Nuisance Functions: Use one of the three outline ML models to obtain estimators $\hat{m}(X_i)$ and $\hat{g}(X_i)$ for the nuisance functions.

\item Compute the Score Function: Evaluate the orthogonal score $\psi(Y_i, T_i, X_i; \hat{\eta})$ using the estimated nuisance parameters.

\item Estimate $\tau$: Solve the empirical moment condition which yields $\hat{\tau}$:

\begin{equation}
\frac{1}{n} \sum_{i=1}^n \psi(Y_i, T_i, X_i; \hat{\eta}) = 0.
\label{eq:orthogonal_score_psi}
\end{equation}
\end{enumerate}

The orthogonal score function mitigates the impact of errors in $\hat{m}(X)$ and $\hat{g}(X)$ on the estimation of $\tau$ while also accomodating the use of modern ML techniques, allowing complex relationships between $Y$, $X$ and $T$.

To prevent overfitting and ensure that the estimation error in the nuisance parameters does not bias the estimator of $\tau$ we employ cross-fitting.

\subsubsection{Cross-Fitting}

The steps of cross-fitting are:

\begin{enumerate}
\item Split the Sample: Divide the data into K folds $\{\mathcal{I}_k\}{k=1}^K$.
\item For Each Fold:
\begin{enumerate}
\item Train Nuisance Estimators: Use data from all other folds:
$$
\mathcal{I}_{-k} = \bigcup{j \neq k} \mathcal{I}_j
$$
to estimate $\hat{m}^{(-k)}(X_i)$ and $\hat{g}^{(-k)}(X_i)$.

\item Compute Score Function: For observations in fold $\mathcal{I}_k$, compute $\psi(Y_i, T_i, X_i; \hat{\eta}^{(-k)})$ using the nuisance estimates from step (a):

\begin{equation}
    \begin{split}
        \psi(Y_i, T_i, X_i; \hat{\eta}^{(-k)}) = & \left( \frac{T_i - \hat{g}^{(-k)}(X_i)}{\hat{\pi}^{(-k)}(X_i)} \right) (Y_i - \hat{m}^{(-k)}(X_i)) \\
        & + \ \hat{m}_1^{(-k)}(X_i) - \hat{m}_0^{(-k)}(X_i) - \tau^{(-k)}.
    \end{split}
    \label{eq:orthogonal_score_fold_k}
\end{equation}

\end{enumerate}
\item Aggregate: Combine the estimates from all folds:

\begin{equation}
\hat{\tau} = \frac{1}{n} \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} \hat{\tau}_i^{(-k)} =  \frac{1}{K} \sum_{k=1}^K \hat{\tau}^{(-k)}
\label{eq:avg_tau_hat}
\end{equation}

\end{enumerate}

\section{Causal Inference Scenarios}

The three most relevant scenarios of causal inference are the ones that require Instrumental Variables, Backdoor Adjustment, or Front Door Adjustment. A common way to represent the causal relation related to those is through the use of DAGs.

\subsection{DAG (Directed Acyclic Graph)}

Directed Acyclic Graph (DAG) serves as a representation of causal assumptions and a tool for deriving statistical properties of the variables involved.

It is composed of nodes (vertices) representing random variables or features and directed edges (arrows), indicating a direct influence or causal effect of $T$ on $Y$.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        >={Stealth[round]},
        every node/.style={draw, circle, minimum size=1cm},
        every edge/.style={draw, ->, thick},
        node distance=2cm
    ]
    
    \node (T) {T};
    \node (Y) [right=of T, xshift=-0.5cm] {Y};
    
    \draw[->] (T) -- (Y);
    \end{tikzpicture}
    \label{fig:dag_example}
    \caption{DAG Example}
\end{figure}

The acyclic characterist is due to absence of directed cycles; that is, there is no path where you can start at a node $X$ and, by following directed edges, return to $X$.

\subsection{Backdoor Adjustment}

A backdoor path from treatment $T$ to the outcome $Y$ represents alternative routes through which association can flow from $T$ to $Y$ that are not due to the causal effect of $T$ on $Y$. In the representation, the confounder $C$ is a common cause of both $T$ and $Y$, thus, a backdoor path.

In such case, the association between $T$ and $Y$ may be partially or entirely due to their mutual dependence on $C$ rather than a direct causal effect, leading to biased causal estimates of the treatment if $C$ is ignored.

The causal effect of $T$ on $Y$ can be expressed using the backdoor adjustment formula:

\begin{equation}
    \mathbb{P}[Y(t)] = \sum_C \mathbb{P}[Y \mid T, C] \mathbb{P}[C],
\end{equation}

Which serves a markov factorization, calculating with respect to the DAG structure.

For the backdoor adjustment to be valid, the following conditions must be satisfied:

\begin{enumerate}
    \item No variable in the adjustment set is a descendant of the treatment $T$.
    \item The adjustment set blocks all backdoor paths from $T$ to $Y$ (backdoor path is any path from $T$ to $Y$ that starts with an arrow into $T$).
\end{enumerate}

DAG~\ref{fig:dag_backdoor_path} illustrates the backdoor path involving the confounder $C$, treatment $T$, outcome $Y$, and features with non-causal association $X_a$ which would not be present in a typical backdoor adjustment DAG, but play a relevant role in the simulations proposed.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        >={Stealth[round]},
        every node/.style={draw, circle, minimum size=1cm},
        every edge/.style={draw, ->, thick},
        node distance=2cm
    ]
    
    \node (T) {T};
    % \node (C) [above=of T, xshift=1.5cm] {C};
    \node (Y) [right=of T, xshift=1cm] {Y};
    \node (XC) [below=of T, xshift=3cm] {$\text{C}$};
    \node (XA) [below=of T, xshift=1cm] {$\text{X}_a$};
    
    % \draw[->] (C) -- (T);
    % \draw[->] (C) -- (Y);
    \draw[->] (T) -- (Y);  
    \draw[->] (XC) -- (T);
    \draw[->] (XC) -- (Y);
    \draw[dotted] (XA) -- (T);
    \draw[dotted] (XA) -- (Y);
    \end{tikzpicture}
    \label{fig:dag_backdoor_path}
    \caption{DAG of Backdoor Path}
\end{figure}

\subsubsection{Backdoor Adjustment Scenario in DML}

In Backdoor Adjustment with DML, the ideal $m(X_i) = \mathbb{E}[Y_i | X_i]$ and $g(X_i) = \mathbb{E}[T_i | X_i]$ from Equations \eqref{eq:m_x_for_target} and \eqref{eq:g_x_for_treatment} are:

\begin{align}
    & m_{\text{BA}}(C_i) = \mathbb{E}[Y_i | C_i],
    \label{eq:m_x_for_target_backdoor_adjustment}
    \\
    & g_{\text{BA}}(C_i) = \mathbb{E}[T_i | C_i],
    \label{eq:g_x_for_treatment_backdoor_adjustment}
\end{align}

In our simulations we also test the habilit of the ML models to estimate ATE under:

\begin{align}
    & m_{\text{BA}}^{\text{w}_1}(C_i, X_{a, i}) = \mathbb{E}[Y_i | C_i, X_{a, i}],
    \label{eq:m_x_for_target_backdoor_adjustment_wrong_1}
    \\
    & g_{\text{BA}}^{\text{w}_1}(C_i, X_{a, i}) = \mathbb{E}[T_i | C_i, X_{a, i}],
    \label{eq:g_x_for_treatment_backdoor_adjustment_wrong_1}
\end{align}

Equations \eqref{eq:m_x_for_target_backdoor_adjustment_wrong_1} and \eqref{eq:g_x_for_treatment_backdoor_adjustment_wrong_1} represent a scenario in which one would not be aware that $X_a$ does not cause $T$ and $Y$.

\subsection{Frontdoor Adjustment}

Frontdoor adjustment is a method used to estimate the causal effect of treatment $T$ on outcome $Y$ when there is unmeasured confounding that cannot be addressed using backdoor adjustment. It leverages a mediator $M$ that lies on the causal path from $T$ to $Y$.

The causal effect of $T$ on $Y$ can be expressed using the frontdoor adjustment formula:

\begin{equation}
\mathbb{P}[Y(t)] = \sum_M \mathbb{P}[M \mid T=t] \sum_{t’} \mathbb{P}[Y \mid M, T=t’] \mathbb{P}[T=t’].
\end{equation}

For instance, the average treatment effect in case $M, T \in \{0, 1\}$:

\begin{equation}
\tau = \left[ P(M = 1 \mid T = 1) - P(M = 1 \mid T = 0) \right] \times \left[ E[Y \mid M = 1] - E[Y \mid M = 0] \right]
\end{equation}

For the frontdoor adjustment to be valid, the following conditions must be satisfied:

\begin{enumerate}
\item All causal paths from $T$ to $Y$ pass through $M$ (i.e., there is no direct effect of $T$ on $Y$ bypassing $M$).
\item There are no unmeasured confounders between $T$ and $M$.
\item All backdoor paths from $M$ to $Y$ are blocked by $T$ (i.e., there are no unmeasured confounders between $M$ and $Y$ that are not affected by $T$).
\end{enumerate}

DAG~\ref{fig:dag_frontdoor_path} illustrates the frontdoor adjustment involving the treatment $T$, mediator $M$, outcome $Y$, observed confounders $C$ and features with no causal association $X_a$. $C$ and $X_a$ would not be present in a typical frontdoor adjustment DAG, but are included due to their relevance in the proposed simulations.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        >={Stealth[round]},
        every node/.style={draw, circle, minimum size=1cm},
        every edge/.style={draw, ->, thick},
        node distance=2cm
    ]
    
    \node (T) {T};
    \node (M) [right=of T, xshift=-0.5cm] {M};
    \node (Y) [right=of M, xshift=-0.5cm] {Y};
    \node (XA) [below=of M, xshift=-1cm] {$\text{X}_a$};
    \node (XC) [below=of M, xshift=1cm] {C};
    
    \draw[->] (T) -- (M);
    \draw[->] (M) -- (Y);
    \draw[->] (XC) -- (T);
    \draw[->] (XC) -- (Y);
    \draw[dotted] (XA) -- (T);
    \draw[dotted] (XA) -- (Y);
    \end{tikzpicture}
    \label{fig:dag_frontdoor_path}
    \caption{DAG of Frontdoor Path}
\end{figure}

\subsubsection{Frontdoor Adjustment Scenario in DML}

In our simulations, we use a binary mediator $M_i \in \{0, 1\}$, similar to the binary treatment $T_i$.

In the Frontdoor Adjustment scenario with DML, we need to account for the mediator $M_i$ when estimating the ATE. The identification of the causal effect involves modeling the relationships between $T_i$, $M_i$, and $Y_i$.

The ideal nuisance functions for DML in this scenario are:

\begin{align}
    & m_{\text{FA}}(M_i, C_i) = \mathbb{E}[Y_i \mid M_i, C_i],
    \label{eq:m_x_for_target_frontdoor_adjustment} \\
    & h_{\text{FA}}(T_i) = \mathbb{E}(M_i \mid T_i),
    \label{eq:h_x_for_mediator_frontdoor_adjustment} \\
    & g_{\text{FA}}(C_i) = \mathbb{E}(T_i \mid C_i),
    \label{eq:g_x_for_treatment_frontdoor_adjustment}
\end{align}

Here $h_{\text{FA}}(M_i, T_i, C_i)$ is the mediator model, representing the probability of the mediator given treatment.

To adapt the orthogonal score function in the presence of the mediator, we modify Equation~\eqref{eq:orthogonal_score} to incorporate the mediator's effect. The adapted orthogonal score function is:

\begin{equation}
\psi(Y_i, T_i, M_i, C_i; \eta) = \left( \frac{T_i - g_{\text{FA}}(C_i)}{g_{\text{FA}}(C_i)(1 - g_{\text{FA}}(C_i))} \right) \left( M_i - h_{\text{FA}}(T_i) \right) \left( Y_i - m_{\text{FA}}(M_i, C_i) \right) + \delta_M \delta_Y(C_i) - \tau
\label{eq:orthogonal_score_frontdoor_adjusted}
\end{equation}

where:

\begin{align}
    \delta_M & = h_{\text{FA}}(T_i = 1) - h_{\text{FA}}(T_i = 0),
    \label{eq:delta_M_Ci} \\
    \delta_Y(C_i) & = m_{\text{FA}}(M_i = 1, C_i) - m_{\text{FA}}(M_i = 0, C_i),
    \label{eq:delta_Y_Ci}
\end{align}

and $\eta$ represents the collection of nuisance functions.

In this score function:

\begin{itemize}
    \item The first term adjusts for the treatment assignment, similar to the original score function, but now includes the mediator.
    \item The product $\left( M_i - h_{\text{FA}}(T_i) \right) \left( Y_i - m_{\text{FA}}(M_i, C_i) \right)$ captures the interaction between the mediator and the outcome.
    \item The term $\delta_M \delta_Y(C_i)$ represents the estimated causal effect based on the mediator and outcome models.
\end{itemize}

In our simulations we also test the hability of the ML models to estimate ATE under the following scenario.

\subsubsection*{Inclusion of $C_i$ and $X_{a, i}$ on every nuisance function}

\begin{align}
    & m_{\text{FA}}^{\text{w}_1}(M_i, C_i, X_{a, i}) = \mathbb{E}[Y_i \mid M_i, C_i, X_{a, i}],
    \label{eq:m_x_for_target_frontdoor_adjustment_wrong_1} \\
    & h_{\text{FA}}^{\text{w}_1}(T_i, C_i, X_{a, i}) = \mathbb{E}(M_i \mid C_i, X_{a, i}),
    \label{eq:h_x_for_mediator_frontdoor_adjustment_wrong_1} \\
    & g_{\text{FA}}^{\text{w}_1}(C_i, X_{a, i}) = \mathbb{E}(T_i \mid C_i, X_{a, i}),
    \label{eq:g_x_for_treatment_frontdoor_adjustment_wrong_1}
\end{align}

Equations \eqref{eq:m_x_for_target_frontdoor_adjustment_wrong_1}, \eqref{eq:h_x_for_mediator_frontdoor_adjustment_wrong_1}, and \eqref{eq:g_x_for_treatment_frontdoor_adjustment_wrong_1} represent a scenario where one is unaware that $X_{a, i}$ has no causal relation to $T_i$, $M_i$, or $Y_i$. Equation \eqref{eq:h_x_for_mediator_frontdoor_adjustment_wrong_1} represents scenarion where one is unaware that $C$ has no causal relation to $M_i$.

\subsubsection*{Ignoring the mediator $M_i$}

\begin{align}
    & m_{\text{FA}}^{\text{w}_2}(C_i, X_{a, i}) = \mathbb{E}[Y_i \mid C_i, X_{a, i}],
    \label{eq:m_x_for_target_frontdoor_adjustment_wrong_2} \\
    & g_{\text{FA}}^{\text{w}_2}(C_i, X_{a, i}) = \mathbb{E}[T_i \mid C_i, X_{a, i}],
    \label{eq:g_x_for_treatment_frontdoor_adjustment_wrong_2}
\end{align}

Equations \eqref{eq:m_x_for_target_frontdoor_adjustment_wrong_2} and \eqref{eq:g_x_for_treatment_frontdoor_adjustment_wrong_2} represent a scenario where the mediator $M_i$ is unavailable or ignored and one is unaware that $X_{a, i}$ has no causal relationship to $T$ or $Y$. In this case, the orthogonal score function is the same as in Equation~\eqref{eq:orthogonal_score}.

\subsubsection*{Considering the mediator $M_i$ as a normal covariate}

\begin{align}
    & m_{\text{FA}}^{\text{w}_3}(C_i, M_i, X_{a, i}) = \mathbb{E}[Y_i \mid C_i, M_i, X_{a, i}],
    \label{eq:m_x_for_target_frontdoor_adjustment_wrong_3} \\
    & g_{\text{FA}}^{\text{w}_3}(C_i, M_i, X_{a, i}) = \mathbb{E}[T_i \mid C_i, M_i, X_{a, i}],
    \label{eq:g_x_for_treatment_frontdoor_adjustment_wrong_3}
\end{align}

Equations \eqref{eq:m_x_for_target_frontdoor_adjustment_wrong_3} to \eqref{eq:g_x_for_treatment_frontdoor_adjustment_wrong_3} also represent scenarios where one is unaware that $X_{a, i}$ has no causal relation to $T_i$ or $Y_i$, and the scenario where $M_i$ is available but not recognized as a mediator. In this case, the orthogonal score function is the same as in Equation~\eqref{eq:orthogonal_score}.

% The estimation procedure involves:

% \begin{enumerate}
%     \item \textbf{Estimate Nuisance Functions}: Use flexible machine learning methods to estimate $\hat{m}_{\text{FA}}(M_i, C_i)$, $\hat{p}_{\text{FA}}(M_i \mid T_i, C_i)$, and $\hat{g}_{\text{FA}}(C_i)$.
    
%     \item \textbf{Compute $\delta_M(C_i)$ and $\delta_Y(C_i)$}:
%     \begin{align}
%         \delta_M(C_i) & = \hat{p}_{\text{FA}}(1 \mid T_i = 1, C_i) - \hat{p}_{\text{FA}}(1 \mid T_i = 0, C_i),
%         \label{eq:delta_M_hat_Ci} \\
%         \delta_Y(C_i) & = \hat{m}_{\text{FA}}(1, C_i) - \hat{m}_{\text{FA}}(0, C_i).
%         \label{eq:delta_Y_hat_Ci}
%     \end{align}
    
%     \item \textbf{Implement Cross-Fitting}: Split the data into folds, estimate the nuisance functions on training folds, and predict on validation folds to avoid overfitting biases.
    
%     \item \textbf{Compute the Orthogonal Score}: Evaluate $\psi(Y_i, T_i, M_i, C_i; \hat{\eta})$ for each observation using the estimated nuisance functions.
    
%     \item \textbf{Estimate $\tau$}: Solve the empirical moment condition:
%     \begin{equation}
%     \frac{1}{n} \sum_{i=1}^n \psi(Y_i, T_i, M_i, C_i; \hat{\eta}) = 0,
%     \label{eq:moment_condition_frontdoor_adjustment}
%     \end{equation}
%     which yields the estimator $\hat{\tau}$.
% \end{enumerate}

% \subsubsection{Frontdoor Adjustment Scenario in DML}

% In our simulations, we use a binary mediator $M \in \{0, 1\}$, similar to $T$.

% In Frontdoor Adjustment with DML, the ideal $m(X_i) = \mathbb{E}[Y_i | X_i]$ and $g(X_i) = \mathbb{E}[T_i | X_i]$ from Equations \eqref{eq:m_x_for_target} and \eqref{eq:g_x_for_treatment} are:

% \begin{align}
%     & m_{\text{FA}}(C_i, M_i) = \mathbb{E}[Y_i | C_i, M_i],
%     \label{eq:m_x_for_target_frontdoor_adjustment}
%     \\
%     & g_{\text{FA}}(C_i) = \mathbb{E}[T_i | C_i],
%     \label{eq:g_x_for_treatment_frontdoor_adjustment}
% \end{align}

% In our simulations we also test the habilit of the ML models to estimate ATE under:

% \begin{align}
%     & m_{\text{FA}}^{\text{w}_1}(C_i, M_i, X_{a, i}) = \mathbb{E}[Y_i | C_i, M_i, X_{a, i}],
%     \label{eq:m_x_for_target_frontdoor_adjustment_wrong_1}
%     \\
%     & g_{\text{FA}}^{\text{w}_1}(C_i, M_i, X_{a, i}) = \mathbb{E}[T_i | C_i, M_i, X_{a, i}],
%     \label{eq:g_x_for_treatment_frontdoor_adjustment_wrong_1}
% \end{align}

% Equation~\eqref{eq:g_x_for_treatment_frontdoor_adjustment_wrong_1} represents a scenario in which one would not be aware that the $T$ causes $M$ but the opposite is not true.

% And under:

% \begin{align}
%     & m_{\text{FA}}^{\text{w}_2}(C_i, X_{a, i}) = \mathbb{E}[Y_i | C_i, X_{a, i}],
%     \label{eq:m_x_for_target_frontdoor_adjustment_wrong_2}
%     \\
%     & g_{\text{FA}}^{\text{w}_2}(C_i, X_{a, i}) = \mathbb{E}[T_i | C_i, X_{a, i}],
%     \label{eq:g_x_for_treatment_frontdoor_adjustment_wrong_2}
% \end{align}

% Equation~\eqref{eq:m_x_for_target_frontdoor_adjustment_wrong_2} represents a scenario in which one would not be aware of the existance of the mediator $M$.

% Equations \eqref{eq:m_x_for_target_frontdoor_adjustment_wrong_1}, \eqref{eq:g_x_for_treatment_frontdoor_adjustment_wrong_1}, \eqref{eq:m_x_for_target_frontdoor_adjustment_wrong_2}, and \eqref{eq:g_x_for_treatment_frontdoor_adjustment_wrong_2} also represent scenarios in which one would not be aware that $X_a$ has no causal relation to $T$ and $Y$.

\subsection{Instrumental Variable}

Instrumental variable (IV) estimation is a method used to estimate the causal effect of a treatment $T$ on an outcome $Y$ when there is unmeasured confounding that cannot be addressed using backdoor or frontdoor adjustments. This method leverages an instrument $Z$, which influences the treatment $T$ but has no direct effect on the outcome $Y$  except through $T$, and is independent of any unmeasured confounders $U$ affecting both $T$ and $Y$.

The causal effect of $T$ on $Y$ can be estimated using the instrumental variable formula:

\begin{equation}
\hat{\tau} = \frac{\text{Cov}(Z, Y)}{\text{Cov}(Z, T)}.
\end{equation}

Alternatively, in terms of expectations for a binary instrument $Z$:

\begin{equation}
\hat{\tau} = \frac{\mathbb{E}[Y \mid Z = 1] - \mathbb{E}[Y \mid Z = 0]}{\mathbb{E}[T \mid Z = 1] - \mathbb{E}[T \mid Z = 0]}.
\end{equation}

For the instrumental variable method to be valid, the following conditions must be satisfied:

\begin{enumerate}
\item Relevance: The instrument $Z$ is associated with the treatment  $T$  (i.e.,  $\text{Cov}(Z, T) \neq 0$  or \( Z \not\!\perp\!\!\!\perp T \)).
\item Exclusion Restriction: The instrument $Z$ affects the outcome $Y$ only through its effect on the treatment $T$ (i.e., there is no direct effect of $Z$ on $Y$ and no other pathways from $Z$ to $Y$ except through  $T$).
\item Independence (Ignorability): The instrument $Z$ is independent of any unmeasured confounders $U$ that affect both $T$ and $Y$ $(i.e.,  Z \perp\!\!\!\perp U )$.
\end{enumerate}

DAG~\ref{fig:dag_instrumental_variable} illustrates the instrumental variable setup involving the unobserved confounder $U$, instrument $Z$, treatment $T$, outcome $Y$, observed confounder $C$, and features with non-causal associations $X_a$. Again, the last two would not be present in a typical instrumental variable DAG, but are included due to their relevance in the proposed simulations.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        >={Stealth[round]},
        every node/.style={draw, circle, minimum size=1cm},
        every edge/.style={draw, ->, thick},
        node distance=2cm
    ]
    
    \node (Z) {Z};
    \node (T) [right=of Z] {T};
    \node (U) [above=of T, xshift=2cm, yshift=-1cm, dashed] {U};
    \node (XC) [below=of T, xshift=1cm] {$\text{C}$};
    \node (XA) [below=of T, xshift=3cm] {$\text{X}_a$};
    \node (Y) [right=of T, xshift=1cm] {Y};
    
    \draw[->] (Z) -- (T);
    \draw[->] (T) -- (Y);
    \draw[->] (U) -- (T);
    \draw[->] (U) -- (Y);
    \draw[->] (XC) -- (T);
    \draw[->] (XC) -- (Y);
    \draw[dotted] (XA) -- (T);
    \draw[dotted] (XA) -- (Y);
    \end{tikzpicture}
    \label{fig:dag_instrumental_variable}
    \caption{DAG of Instrumental Variable}
\end{figure}
    
\subsubsection{Instrumental Variable Scenario in DML}

This formulation adjusts for the binary nature of $T_i$ and incorporates the propensity scores directly.

In Instrumental Variable Scenario with DML, the ideal $m(X_i) = \mathbb{E}[Y_i | X_i]$ and $g(X_i) = \mathbb{E}[T_i | X_i]$ from Equations \eqref{eq:m_x_for_target} and \eqref{eq:g_x_for_treatment} are:

\begin{align}
    & m_{\text{IV}}(C_i) = \mathbb{E}[Y_i | C_i],
    \label{eq:m_x_for_target_instrumental_variable}
    \\
    & g_{\text{IV}}(C_i, Z_i) = \mathbb{E}[T_i | C_i, Z_i],
    \label{eq:g_x_for_treatment_instrumental_variable}
\end{align}

The orthogonal score function from \eqref{eq:orthogonal_score} becomes:

\begin{equation}
    \psi_{\text{IV}}(Y_i, T_i, Z_i, X_i; \theta, \eta) = \left( \frac{T_i - g(X_i)}{g(X_i)(1 - g(X_i))} \right) \left( Y_i - m(X_i) - \tau [T_i - g(X_i)] \right) \cdot [g(Z_i, X_i) - g(X_i)].
\label{eq:orthogonal_score_iv_binary}
\end{equation}

In our simulations we also test the habilit of the ML models to estimate ATE under:

\begin{align}
    & m_{\text{IV}}^{\text{w}}(C_i, Z_i, X_{a, i}) = \mathbb{E}[Y_i | C_i, Z_i, X_{a, i}],
    \label{eq:m_x_for_target_instrumental_variable_wrong}
    \\
    & g_{\text{IV}}^{\text{w}}(C_i, Z_i, X_{a, i}) = \mathbb{E}[T_i | C_i, Z_i, X_{a, i}],
    \label{eq:g_x_for_treatment_instrumental_variable_wrong}
\end{align}

Equation~\eqref{eq:m_x_for_target_instrumental_variable_wrong} represents a scenario in which one would not be aware that the the instrument $Z$ only has a causal relation to $T$. Equations \eqref{eq:m_x_for_target_instrumental_variable_wrong} and \eqref{eq:g_x_for_treatment_instrumental_variable_wrong} represent a scenario in which one would not be aware that $X_a$ has no causal relation to $T$ and $Y$.

\subsubsection{Instrumental Variable Scenario in DML}

In the Instrumental Variable (IV) scenario with DML, we aim to estimate the causal effect of the treatment $T_i$ on the outcome $Y_i$ using an instrument $Z_i$ that affects $T_i$ but has no direct effect on $Y_i$ except through $T_i$. The presence of unobserved confounders $U_i$ that affect both $T_i$ and $Y_i$ violates the Conditional Independence Assumption, necessitating the use of instrumental variables.

To adapt the DML framework to the IV setting, we need to define appropriate nuisance functions and construct an orthogonal score function suitable for the IV context.

The ideal nuisance functions in this scenario are:

\begin{align}
    & m_{\text{IV}}(X_i) = \mathbb{E}[Y_i \mid X_i],
    \label{eq:m_x_for_target_instrumental_variable} \\
    & q_{\text{IV}}(Z_i, X_i) = \mathbb{E}[T_i \mid Z_i, X_i],
    \label{eq:q_z_for_instrument_instrumental_variable} \\
    & g_{\text{IV}}(X_i) = \mathbb{E}[T_i \mid X_i],
    \label{eq:g_x_for_treatment_instrumental_variable}
\end{align}

Here:

- $m_{\text{IV}}(X_i)$ is the outcome model, capturing the expected outcome given covariates.
- $q_{\text{IV}}(Z_i, X_i)$ is the instrument propensity score, representing the expected treatment given the instrument and covariates.
- $g_{\text{IV}}(X_i)$ is the treatment model, representing the expected treatment given covariates.

The orthogonal score function for the IV scenario is different from the standard DML orthogonal score function. An appropriate orthogonal score function in the linear IV context is:

\begin{equation}
    \psi_{\text{IV}}(W_i; \theta, \eta) = \left( Y_i - m_{\text{IV}}(X_i) - \theta [T_i - g_{\text{IV}}(X_i)] \right) \left( q_{\text{IV}}(Z_i, X_i) - g_{\text{IV}}(X_i) \right),
    \label{eq:orthogonal_score_iv}
\end{equation}

where:

- $W_i = (Y_i, T_i, Z_i, X_i)$.
- $\theta$ is the parameter of interest (the causal effect of $T_i$ on $Y_i$).
- $\eta$ represents the collection of nuisance functions.

This score function leverages the variation in $T_i$ induced by the instrument $Z_i$ while controlling for $X_i$. It satisfies the Neyman orthogonality condition, making it robust to estimation errors in the nuisance functions.

The estimation procedure involves:

\begin{enumerate}
    \item \textbf{Estimate Nuisance Functions}: Use flexible machine learning methods to estimate $\hat{m}_{\text{IV}}(X_i)$, $\hat{q}_{\text{IV}}(Z_i, X_i)$, and $\hat{g}_{\text{IV}}(X_i)$.
    
    \item \textbf{Implement Cross-Fitting}: Split the data into folds, estimate the nuisance functions on training folds, and predict on validation folds to mitigate overfitting biases.
    
    \item \textbf{Compute the Orthogonal Score}: Evaluate $\psi_{\text{IV}}(W_i; \theta, \hat{\eta})$ for each observation using the estimated nuisance functions.
    
    \item \textbf{Estimate $\theta$}: Solve the empirical moment condition:
    \begin{equation}
        \frac{1}{n} \sum_{i=1}^n \psi_{\text{IV}}(W_i; \hat{\theta}, \hat{\eta}) = 0,
        \label{eq:moment_condition_iv}
    \end{equation}
    which yields the estimator $\hat{\theta}$.
\end{enumerate}

In our simulations, we also test the ability of the ML models to estimate the causal effect under different scenarios:

\begin{align}
    & m_{\text{IV}}^{\text{w}}(X_i, Z_i, X_{a,i}) = \mathbb{E}[Y_i \mid X_i, Z_i, X_{a,i}],
    \label{eq:m_x_for_target_instrumental_variable_wrong} \\
    & q_{\text{IV}}^{\text{w}}(Z_i, X_i, X_{a,i}) = \mathbb{E}[T_i \mid Z_i, X_i, X_{a,i}],
    \label{eq:q_z_for_instrument_instrumental_variable_wrong} \\
    & g_{\text{IV}}^{\text{w}}(X_i, X_{a,i}) = \mathbb{E}[T_i \mid X_i, X_{a,i}],
    \label{eq:g_x_for_treatment_instrumental_variable_wrong}
\end{align}

These equations represent scenarios where irrelevant variables $X_{a,i}$, which have no causal relation to $T_i$ or $Y_i$, are included in the models. Equation~\eqref{eq:m_x_for_target_instrumental_variable_wrong} also represents a scenario where one incorrectly includes the instrument $Z_i$ in the outcome model, violating the exclusion restriction.

For the instrumental variable method to be valid, the following conditions must be satisfied:

\begin{enumerate}
    \item \textbf{Relevance}: The instrument $Z_i$ is associated with the treatment $T_i$ (i.e., $\mathbb{E}[T_i \mid Z_i, X_i] \neq \mathbb{E}[T_i \mid X_i]$).
    
    \item \textbf{Exclusion Restriction}: The instrument $Z_i$ affects the outcome $Y_i$ only through its effect on the treatment $T_i$, and not directly or through any other pathways.
    
    \item \textbf{Independence (Ignorability)}: The instrument $Z_i$ is independent of any unmeasured confounders $U_i$ that affect both $T_i$ and $Y_i$, conditional on covariates $X_i$.
\end{enumerate}

By appropriately specifying and estimating the nuisance functions, and constructing the orthogonal score function as in Equation~\eqref{eq:orthogonal_score_iv}, the DML framework can provide consistent and asymptotically normal estimates of the causal effect in the IV scenario, even in the presence of high-dimensional covariates and complex relationships among variables.

\end{document}
